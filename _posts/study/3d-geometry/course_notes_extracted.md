CS231A Course Notes 4: Stereo Systems and
Structure from Motion
Kenji Hata and Silvio Savarese
1 Introduction
In the previous notes, we covered how adding additional viewpoints of a
scene can greatly enhance our knowledge of the said scene. We focused on
the epipolar geometry setup in order to relate points of one image plane to
points in the other without extracting any information about the 3D scene.
In these lecture notes, we will discuss how to recover information about the
3D scene from multiple 2D images.
2 Triangulation
One of the most fundamental problems in multiple view geometry is the
problem of triangulation , the process of determining the location of a 3D
point given its projections into two or more images.
Figure 1: The setup of the triangulation problem when given two views.
1

In the triangulation problem with two views, we have two cameras with
known camera intrinsic parameters KandK′respectively. We also know the
relative orientations and offsets R, T of these cameras with respect to each
other. Suppose that we have a point Pin 3D, which can be found in the
images of the two cameras at pandp′respectively. Although the location of
Pis currently unknown, we can measure the exact locations of pandp′in
the image. Because K, K′, R, T are known, we can compute the two lines of
sight ℓandℓ′, which are defined by the camera centers O1, O2and the image
locations p, p′. Therefore, Pcan be computed as the intersection of ℓandℓ′.
Figure 2: The triangulation problem in real-world scenarios often involves
minimizing the reprojection error.
Although this process appears both straightforward and mathematically
sound, it does not work very well in practice. In the real world, because the
observations pandp′are noisy and the camera calibration parameters are
not precise, finding the intersection point of ℓandℓ′may be problematic. In
most cases, it will not exist at all, as the two lines may never intersect.
2.1 A linear method for triangulation
In this section, we describe a simple linear triangulation method that solves
the lack of an intersection point between rays. We are given two points in the
images that correspond to each other p=MP = (x, y,1) and p′=M′P=
(x′, y′,1). By the definition of the cross product, p×(MP) = 0. We can
2

explicitly use the equalities generated by the cross product to form three
constraints:
x(M3P)−(M1P) = 0
y(M3P)−(M2P) = 0
x(M2P)−y(M1P) = 0(2.1)
where Miis the i-th row of the matrix M. Similar constraints can be for-
mulated for p′andM′. Using the constraints from both images, we can
formulate a linear equation of the form AP= 0 where
A=
xM 3−M1
yM3−M2
x′M′
3−M′
1
y′M′
3−M′
2
(2.2)
This equation can be solved using SVD to find the best linear estimate of
the point P. Another interesting aspect of this method is that it can actu-
ally handle triangulating from multiple views as well. To do so, one simply
appends additional rows to Acorresponding to the added constraints by the
new views.
This method, however is not suitable for projective reconstruction, as it is
not projective-invariant. For example, suppose we replace the camera matri-
cesM, M′with ones affected by a projective transformation MH−1, M′H−1.
The matrix of linear equations Athen becomes AH−1. Therefore, a solution
Pto the previous estimation of AP= 0 will correspond to a solution HPfor
the transformed problem ( AH−1)(HP) = 0. Recall that SVD solves for the
constraint that ∥P∥= 1, which is not invariant under a projective transfor-
mation H. Therefore, this method, although simple, is often not the optimal
solution to the triangulation problem. -
2.2 A nonlinear method for triangulation
Instead, the triangulation problem for real-world scenarios is often mathe-
matically characterized as solving a minimization problem:
min
ˆP∥MˆP−p∥2+∥M′ˆP−p′∥2(2.3)
In the above equation, we seek to find a ˆPin 3D that best approximates P
by finding the best least-squares estimate of the reprojection error ofˆPin
both images. The reprojection error for a 3D point in an image is the distance
between the projection of that point in the image and the corresponding
3

observed point in the image plane. In the case of our example in Figure 2,
since Mis the projective transformation from 3D space to image 1, the
projected point of ˆPin image 1 is MˆP. The matching observation of ˆP
in image 1 is p. Thus, the reprojection error for point P in image 1 is the
distance ∥MˆP−p∥. The overall reprojection error found in Equation 2.3
is the sum of the reprojection errors across all the points in the image. For
cases with more than two images, we would simply add more distance terms
to the objective function.
min
ˆPX
i∥MˆPi−pi∥2(2.4)
In practice, there exists a variety of very sophisticated optimization tech-
niques that result in good approximations to the problem. However, for the
scope of the class, we will focus on only one of these techniques, which is the
Gauss-Newton algorithm for nonlinear least squares. The general nonlinear
least squares problem is to find an x∈Rnthat minimizes
∥r(x)∥2=mX
i=1ri(x)2(2.5)
where ris any residual function r:Rn→Rmsuch that r(x) =f(x)−
yfor some function f, input x, and observation y. The nonlinear least
squares problem reduces to the regular, linear least squares problem when
the function fis linear. However, recall that, in general, our camera matrices
are not affine. Because the projection into the image plane often involves a
division by the homogeneous coordinate, the projection into the image is
generally nonlinear.
Notice that if we set eito be a 2 ×1 vector ei=MˆPi−pi, then we can
reformulate our optimization problem to be:
min
ˆPX
iei(ˆP)2(2.6)
which can be perfectly represented as a nonlinear least squares problem.
In these notes, we will cover how we can use the popular Gauss-Newton
algorithm to find an approximate solution to this nonlinear least squares
problem. First, let us assume that we have a somewhat reasonable estimate
of the 3D point ˆP, which we can compute by the previous linear method.
The key insight of the Gauss-Newton algorithm is to update our estimate by
correcting it towards an even better estimate that minimizes the reprojection
error. At each step we want to update our estimate ˆPby some δP:ˆP=
ˆP+δP.
4

But how do we choose the update parameter δP? The key insight of the
Gauss-Newton algorithm is to linearize the residual function near the current
estimate ˆP. In the case of our problem, this means that the residual error e
of a point Pcan be thought of as:
e(ˆP+δP)≈e(ˆP) +∂e
∂PδP (2.7)
Subsequently, the minimization problem transforms into
min
δP∥∂e
∂PδP−(−e(ˆP))∥2(2.8)
When we formulate the residual like this, we can see that it takes the format
of the standard linear least squares problem. For the triangulation problem
with Nimages, the linear least squares solution is
δP=−(JTJ)−1JTe (2.9)
where
e=
e1
...
eN
=
p1−M1ˆP
...
pn−MnˆP
 (2.10)
and
J=
∂e1
∂ˆP1∂e1
∂ˆP2∂e1
∂ˆP3.........
∂eN
∂ˆP1∂eN
∂ˆP2∂eN
∂ˆP3
(2.11)
Recall that the residual error vector of a particular image eiis a 2 ×1
vector because there are two dimensions in the image plane. Consequently,
in the simplest two camera case ( N= 2) of triangulation, this results in the
residual vector ebeing a 2 N×1 = 4×1 vector and the Jacobian Jbeing a
2N×3 = 4×3 matrix. Notice how this method handles multiple views seam-
lessly, as additional images are accounted for by adding the corresponding
rows to the evector and Jmatrix. After computing the update δP, we can
simply repeat the process for a fixed number of steps or until it numerically
converges. One important property of the Gauss-Newton algorithm is that
our assumption that the residual function is linear near our estimate gives
us no guarantee of convergence. Thus, it is always useful in practice to put
an upper bound on the number of updates made to the estimate.
5

3 Affine structure from motion
At the end of the previous section, we hinted how we can go beyond two views
of a scene to gain information about the 3D scene. We will now explore the
extension of the geometry of two cameras to multiple cameras. By combining
observations of points from multiple views, we will be able to simultaneously
determine both the 3D structure of the scene and the parameters of the
camera in what is known as structure from motion .
Figure 3: The setup of the general structure from motion problem.
Here, we formally introduce the structure from motion problem. Suppose
we have mcameras with camera transformations Miencoding both the in-
trinsic and extrinsic parameters for the cameras. Let Xjbe one of the n3D
points in the scene. Each 3D point may be visible in multiple cameras at the
location xij, which is the projection of Xjto the image of the camera iusing
the projective transformation Mi. The aim of structure from motion is to
recover both the structure of the scene (the n3D points Xj) and the motion
of the cameras (the mprojection matrices Mi) from all the observations xij.
3.1 The affine structure from motion problem
Before tackling the general structure from motion problem, we will first start
with a simpler problem, which assumes the cameras are affine or weak per-
spective. Ultimately, the lack of the perspective scaling operation makes the
6

mathematical derivation easier for this problem.
Previously, we derived the above equations for perspective and weak per-
spective cases. Remember that in the full perspective model, the camera
matrix is defined as
M=A b
v1
(3.1)
where vis some non-zero 1 ×3 vector. On the other hand, for the weak
perspective model, v= 0. We find that this property makes the homogeneous
coordinate of MX equal to 1:
x=MX =
m1
m2
0 0 0 1

X1
X2
X3
1
=
m1X
m2X
1
 (3.2)
Consequently, the nonlinearity of the projective transformation disap-
pears as we move from homogeneous to Euclidean coordinates, and the weak
perspective transformation acts as a mere magnifier. We can more compactly
represent the projection as:
m1X
m2X
=
A b
X=AX+b (3.3)
and represent any camera matrix in the format Maffine=
A b
. Thus, we
now use the affine camera model to express the relationship from a point Xj
in 3D and the corresponding observations in each affine camera (for instance,
xijin camera i).
Returning to the structure from motion problem, we need to estimate m
matrices Mi, and the nworld coordinate vectors Xj, for a total of 8 m+ 3n
unknowns, from mnobservations. Each observation creates 2 constraints
per camera, so there are 2 mnequations in 8 m+ 3nunknowns. We can
use this equation to know the lower bound on the number of corresponding
observations in each of the images that we need to have. For example, if
we have m= 2 cameras, then we need to have at least n= 16 points in
3D. However, once we do have enough corresponding points labeled in each
image, how do we solve this problem?
3.2 The Tomasi and Kanade factorization method
In this part, we outline Tomasi and Kanade’s factorization method for
solving the affine structure from motion problem. This method consists of
two major steps: the data centering step and the actual factorization step.
7

Figure 4: When applying the centering step, we translate all of the image
points such that their centroid (denoted as the lower left red cross) is located
at the origin in the image plane. Similarly, we place the world coordinate
system such that the origin is at the centroid of the 3D points (denoted as
the upper right red cross).
Let’s begin with the data centering step. In this step, the main idea is
center the data at the origin. To do so, for each image i, we redefine new
coordinates ˆ xijfor each image point xijby subtracting out their centroid ¯ xi:
ˆxij=xij−¯xi=xij−1
nnX
j=1xij (3.4)
Recall that the affine structure from motion problem allows us to define the
relationship between image points xij, the camera matrix variables Aiand
bi, and the 3D points Xjas:
xij=AiXj+bi (3.5)
After this centering step, we can combine definition of the centered image
8

points ˆ xijin Equation 3.4 and the affine expression in Equation 3.5:
ˆxij=xij−1
nnX
k=1xik
=AiXj−1
nnX
k=1AiXk
=Ai(Xj−1
nnX
k=1Xk)
=Ai(Xj−¯X)
=AiˆXj(3.6)
As we see from Equation 3.6, if we translate the origin of the world ref-
erence system to the centroid ¯X, then the centered coordinates of the image
points ˆ xijand centered coordinates of the 3D points ˆXijare related only by
a single 2 ×3 matrix Ai. Ultimately, the centering step of the factorization
method allows us to create a compact matrix product representation to relate
the 3D structure with their observed points in multiple images.
However, notice that in the matrix product ˆ xij=AiˆXj, we only have
access to the values on the left hand side of the equation. Thus, we must
somehow factor out the motion matrices Aiand structure Xj. Using all
the observations for all the cameras, we can build a measurement matrix D,
made up of nobservations in the mcameras (remember that each ˆ xijentry
is a 2x1 vector):
D=
ˆx11ˆx12. . . ˆx1n
ˆx21ˆx22. . . ˆx2n
...
ˆxm1ˆxm2. . .ˆxmn
(3.7)
Now recall that because of our affine assumption, Dcan be expressed as
the product of the 2 m×3 motion matrix M(which comprises the camera
matrices A1, . . . A m) and the 3 ×nstructure matrix S(which comprises the
3D points X1, . . . X n). An important fact that we will use is that rank( D) = 3
since Dis the product of two matrices whose max dimension is 3.
To factorize DintoMandS, we will use the singular value decomposition,
D=UΣVT. Since we know the rank( D) = 3, so there will only be 3 non-
zero singular values σ1, σ2, and σ3in Σ. Thus, we can further reduce the
9

expression and obtain the following decomposition:
D=UΣVT
=
u1. . . u n
σ10 0 0 . . .0
0σ20 0 . . .0
0 0 σ30. . .0
0 0 0 0 . . .0
...
0 0 0 0 . . .0

vT
1...
vT
n

=
u1u2u3
σ10 0
0σ20
0 0 σ3

vT
1
vT
2
vT
3

=U3Σ3VT
3(3.8)
In this decomposition, Σ 3is defined as the diagonal matrix formed by the
non-zero singular values, while U3andVT
3are obtained by taking the corre-
sponding three columns of Uand rows of VTrespectively. Unfortunately, in
practice, rank( D)>3 because of measurement noise and the affine camera
approximation. However, recall that when rank( D)>3,U3W3VT
3is still
the best possible rank-3 approximation of MSin the sense of the Frobenius
norm.
Upon close inspection, we see that the matrix product Σ 3VT
3forms a 3 ×n
matrix, which exactly the same size as the structure matrix S. Similarly, U3
is a 2m×3 matrix, which is the same size as the motion matrix M. While this
way of associating the components of the SVD decomposition to MandS
leads to a physically and geometrical plausible solution of the affine structure
from motion problem, this choice is not a unique solution. For example, we
could also set the motion matrix to M=U3Σ3and the structure matrix to
S=VT
3, since in either cases the observation matrix Dis the same. So what
factorization do we choose? In their paper, Tomasi and Kanade concluded
that a robust choice of the factorization is M=U3√Σ3andS=√Σ3VT
3.
3.3 Ambiguity in reconstruction
Nevertheless, we find inherent ambiguity in any choice of the factorization
D=MS, as any arbitrary, invertible 3 ×3 matrix Amay be inserted into
the decomposition:
D=MAA−1S= (MA)(A−1S) (3.9)
This means that the camera matrices obtained from motion Mand the 3D
points obtained from structure Sare determined up to a multiplication by a
10

common matrix A. Therefore, our solution is underdetermined, and requires
extra constraints to resolve this affine ambiguity. When a reconstruction has
affine ambiguity, it means that parallelism is preserved, but the metric scale
is unknown.
Another important class of ambiguities for reconstruction is the similarity
ambiguity, which occurs when a reconstruction is correct up to a similarity
transform (rotation, translation and scaling). A reconstruction with only
similarity ambiguity is known as a metric reconstruction. This ambiguity
exists even when the camera are intrinsically calibrated. The good news is
that for calibrated cameras, the similarity ambiguity is the only ambiguity1.
The fact that there is no way to recover the absolute scale of a scene from
images is fairly intuitive. An object’s scale, absolute position and canonical
orientation will always be unknown unless we make further assumptions (e.g,
we know the height of the house in the figure) or incorporate more data.
This is because some attributes may compensate for others. For instance,
to get the same image, we can simply move the object backwards and scale
it accordingly. One such example of removing similarity ambiguity occurred
during the camera calibration procedure, where we made the assumption
that we know the location of the calibration points with respect to the world
reference system. This enabled us to know the size of the squares of the
checkerboard to learn a metric scale of the 3D structure.
4 Perspective structure from motion
After studying the simplified affine structure from motion problem, let us
now consider the general case for projective cameras Mi. In the general
case with projective cameras, each camera matrix Micontains 11 degrees of
freedom, as it is defined up to scale:
Mi=
a11a12a13b1
a21a22a23b2
a31a32a331
 (4.1)
Moreover, similar to the affine case where the solution can be found up
to an affine transformation, solutions for structure and motion can be de-
termined up a projective transformation in the general case: we can always
arbitrarily apply a 4 ×4 projective transformation Hto the motion matrix, as
long as we also transform the structure matrix by the inverse transformation
H−1. The resulting observations in the image plane will still be the same.
1See [Longuet-Higgins ’81] for more details.
11

Similar to the affine case, we can set up the general structure from motion
problem as estimating both the mmotion matrices Miandn3D points Xj
from mnobservations xij. Because cameras and points can only be recovered
up to a 4 ×4 projective transformation up to scale (15 parameters), we
have 11 m+ 3n−15 unknowns in 2 mnequations. From these facts, we can
determine the number of views and observations that are required to solve
for the unknowns.
4.1 The algebraic approach
Figure 5: In the algebraic approach, we consider sequential, camera pairs to
determine camera matrices M1andM2up to a perspective transformation.
We then find a perspective transformation Hsuch that M1H= [I0] and
M2H= [A B ]
.
We will now cover the algebraic approach , which leverages the concept
of fundamental matrix Ffor solving the structure from motion problem for
two cameras. As shown in Figure 5, the main idea of the algebraic approach
is to compute two camera matrices M1andM2, which can only be computed
up to a perspective transformation H. Since each Mican only be computed
up a perspective transformation H, we can always consider a Hsuch that
the first camera projection matrix M1H−1is canonical. Of course, the same
transformation must also be applied to the second camera which lead to the
form shown:
12

M1H−1= [I0] M2H−1= [A b] (4.2)
In order to accomplish this task, we must first compute the fundamental
matrix Fusing the eight point algorithm covered in the previous course
notes. We now will use Fto estimate the projective camera matrices M1
andM2. In order to do this estimation, we define Pto be the corresponding
3D point for the corresponding observations in the images pandp′. Since we
have applied H−1to both camera projection matrices, we must also apply
Hto the structure, giving us eP=HP. Therefore, we can relate the pixel
coordinates pandp′to the transformed structure as follows:
p=M1P=M1H−1HP= [I|0]eP
p′=M2P=M2H−1HP= [A|b]eP(4.3)
An interesting property between the two image correspondences pandp′
occur by some creative substitutions:
p′= [A|b]eP
=A[I|0]eP+b
=Ap+b(4.4)
Using Equation 4.4, we can write the cross product between p′andbas:
p′×b= (Ap+b)×b=Ap×b (4.5)
By the definition of cross product, p′×bis perpendicular to p′. Therefore,
we can write:
0 =p′T(p′×b)
=p′T(Ap×b)
=p′T·(b×Ap)
=p′T[b]×Ap(4.6)
Looking at this constraint, it should remind you of the general definition of
the Fundamental matrix p′TFp= 0. If we set F= [b]×A, then extracting A
andbsimply breaks down to a decomposition problem.
Let us begin by determining b. Again, using the definition of cross prod-
uct, we can write,
F⊤b= [[b]×A]⊤b= 0 (4.7)
13

. Since Fis singular, bcan be computed as a least square solution of F⊤b= 0,
with∥b∥= 1, using SVD.
Once bis known, we can now compute A. If we set A=−[b]×F, then we
can verify that this definition satisfies F= [b]×A:
[b×]A′=−[b×][b×]F
= (bbT− |b|2I)F
=bbTF+|b|2F
= 0 + 1 ·F
=F(4.8)
Consequently, we determine the two expressions for our camera matrices
M1H−1andM2H−1:
˜M1= [I0] ˜M2= [−[b×]F b] (4.9)
Before we conclude this section, we want give a geometrical interpretation
forb. We know bsatisfies Fb= 0. Remember the epipolar constraints we
derived in the previous course notes, which found that the epipoles in an
image are the points that map to zero when transformed by the Fundamental
matrix (i.e. Fe2= 0 and FTe1= 0). We can see, therefore, that bis
an epipole. This provides a new set of equations for the camera projection
matrices (Eqs. 4.10).
˜M1= [I0] ˜M2= [−[e×]F e] (4.10)
4.2 Determining motion from the Essential matrix
One useful way of improving the reconstruction obtained by the algebraic
approach is to use calibrated cameras. We can extract a more accurate,
initial estimate of camera matrices by using the Essential matrix, which is a
special case of the Fundamental matrix for normalized coordinates. Recall
that, by using the Essential matrix E, we make an assumption that we have
calibrated the camera and thus know the intrinsic camera matrix K. We
can either compute the Essential matrix Eeither from the normalized image
coordinates directly or from its relationship with the Fundamental matrix F
and intrinsic matrix K:
E=KTFK (4.11)
Because the Essential matrix assumes that we have calibrated cameras,
we should remember that it only has five degrees of freedom, as it only
14

encodes the extrinsic parameters: the rotation Rand translation tbetween
the cameras. Luckily, this is exactly the information that we want to extract
to create our motion matrix. First, recall that the Essential matrix Ecan
be represented as
E= [t]×R (4.12)
As such, perhaps we can find a strategy to factor Einto its two components.
First, we should notice that the cross product matrix [ t]×is skew-symmetric.
We define two matrices that we will use in the decomposition:
W=
0−1 0
1 0 0
0 0 1
, Z =
0 1 0
−1 0 0
0 0 0
 (4.13)
One important property we will use later is that Z= diag(1 ,1,0)Wup to a
sign. Similarly, we will also use the fact that ZW=ZWT= diag(1 ,1,0) up
to a sign.
As a result of Jordan decomposition, we can create a block decomposition
of a general skew-symmetric matrix known up to scale. Thus, we can write
[t]×as
[t]×=UZUT(4.14)
where Uis some orthogonal matrix. Therefore, we can rewrite the decom-
position as:
E=Udiag(1 ,1,0)(WUTR) (4.15)
Looking at this expression carefully, we see that it closely resembles the
singular value decomposition E=UΣVT, where Σ contains two equal sin-
gular values. If we know Eup to scale and we assume that it takes the form
E=Udiag(1 ,1,0)VT, then we arrive at the following factorizations of E:
[t]×=UZUT, R =UWVTorUWTVT(4.16)
We can prove that the given factorizations are valid by inspection. We can
also prove that there are no other factorizations. The form of [ t]×is deter-
mined by the fact that its left null space must be the same as the null space
ofE. Given unitary matrices UandV, any rotation Rcan be decomposed
intoUXVTwhere Xis another rotation matrix. After substituting these
values in, we get ZX= diag(1 ,1,0) up to scale. Thus, Xmust be equal to
WorWT.
Note that this factorization of Eonly guarantees that the matrices UWVT
orUWTVTis orthogonal. To ensure that Ris a valid rotations, we simply
make sure that the determinant of Ris positive:
R= (det UWVT)UWVTor (det UWTVT)UWTVT(4.17)
15

Similar to how the rotation Rcan take on two potential values, the trans-
lation vector tcan also take on several values. From the definition of cross
product, we know that
t×t= [t]×t=UZUTt= 0 (4.18)
Knowing that Uis unitary, we can find that the ∥[t]×∥F=√
2. Therefore,
our estimate of tfrom this factorization will come from the above equation
and the fact that Eis known up to scale. This means that
t=±U
0
0
1
=±u3 (4.19)
where u3is the third column of U. By inspection, we can also verify that we
get the same results by reformatting [ t]×=UZUTinto the vector tknown
up to a sign.
Figure 6: There are four possible solutions for extracting the relative camera
rotation Rand translation tfrom the Essential matrix. However, only in
(a) is the reconstructed point in front of both of the cameras. (Figure taken
from Hartley and Zisserman textbook page 260)
As illustrated in Figure 6, there are four potential R, tpairings since there
exists two options for both Randt. Intuitively, the four pairings include all
16

possible pairings of rotating a camera in a certain direction or rotating the
camera in the opposite direction combined with the option of translating
it in a certain direction or the opposite direction. Therefore, under ideal
conditions, we would only need to triangulate one point to determine the
correct R, tpair. For the correct R, tpair, the triangulated point ˆPexists in
front of both cameras, which means that it has a positive z-coordinate with
respect to both camera reference systems. Due to measurement noise, we
often do not rely on triangulating only one point, but will instead triangulate
many points and determine the correct R, tpair as the one that contains the
most of these points in front of both cameras.
5 An example structure from motion pipeline
After finding the relative motion matrices Mi, we can use them to determine
the world coordinates of the points Xj. In the case of the algebraic method,
the estimate of such points will be correct up to the perspective transfor-
mation H. In extracting the camera matrices from the Essential matrix,
the estimates can be known up to scale. In both cases, the 3D points can be
computed from the estimated camera matrices via the triangulation methods
described earlier.
The extension to the multi-view case can be done by chaining pairwise
cameras. We can use the algebraic approach or the Essential matrix to obtain
solutions for the camera matrices and the 3D points for any pair of cameras,
provided that there are enough point correspondences. The reconstructed
3D points are associated to the point correspondences available between the
camera pair. Those pairwise solutions may be combined together (optimized)
in a approach called bundle adjustment as we will see next.
5.1 Bundle adjustment
There are major limitations related to the previous methods for solving the
structure from motion problem that we have discussed so far. The factoriza-
tion method assumes that all points are visible in every image. This is very
unlikely to happen because of occlusions and failures to find correspondences
when we either have many images or some of the images were taken far
apart. Finally the algebraic approach produces pairwise solutions that can
be combined into a camera chain, but does not solve for a coherent optimized
reconstruction using all the cameras and 3D points.
To address these limitations, we introduce bundle adjustment , which
is a nonlinear method for solving the structure from motion problem. In the
17

optimization, we aim to minimize the reprojection error, which is the pixel
distance between the projection of a reconstructed point into the estimated
cameras and its corresponding observations for all the cameras and for all
the points. Previously, when discussing nonlinear optimization methods for
triangulation, we focused primarily on the two camera case, in which we nat-
urally assumed that each camera saw all the correspondences between the
two. However, since bundle adjustment handles several cameras, it only cal-
culates the reprojection error for only the observations that can be seen by
each camera. Ultimately though, this optimization problem is very similar
to the one we introduced when talking about nonlinear methods for triangu-
lation.
Two common approaches for solving bundle adjustment’s nonlinear opti-
mization include the Gauss-Newton algorithm and the Levenberg-Marquardt
algorithm. You can refer to the previous section about details on the Gauss-
Newton algorithm and refer to the Hartley and Zisserman textbook for more
details on the Levenberg-Marquardt algorithm.
In conclusion, bundle adjustment has some important advantages and
limitations when compared to the other methods we have surveyed. It is par-
ticularly useful because it can handle a large number of views smoothly and
also handle cases when particular points are not observable by every image.
However, the main limitation is that it is a particularly large minimization
problem, as the parameters grow with the number of views. Additionally,
it requires a good initial condition since it relies on nonlinear optimization
techniques. For this reason, bundle adjustment is often used as final step of
most structure from motion implementations (i.e., after the factorization or
algebraic approach), as a factorization or algebraic approach may provide a
good initial solution for the optimization problem.
18