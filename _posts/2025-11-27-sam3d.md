---
title: "[PaperReview] SAM 3D: 3Dfy Anything in Images"
categories: [3D Vision]
tags: [3D Vision, SAM3D, Image-to-3D, PaperReview]
article_header:
  type: overlay
  theme: dark
  background_color: '#0d1b2a'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(13, 27, 42, .85), rgba(176, 125, 103, .65))'
    src: /assets/images/posts/sam3d/figure1.png
---

<!--more-->

## 1. Introduction

이 논문이 하는 말 한 줄로 요약하면:

> **"SAM 3D는 자연 이미지 한 장 + 객체 마스크만으로, 그 객체의 3D geometry + texture + 3D 공간에서의 위치(layout)까지 한 번에 뽑는 foundation model이다."**  

기존 single-view 3D 방법들은 보통:

- **깨끗한 배경, 단일 오브젝트, synthetic 데이터**에 강하고
- **실제 사진(in-the-wild)**에서:
  - clutter, occlusion, 이상한 각도, 조명 문제 때문에 성능이 크게 떨어짐
  - 무엇보다 **실세계 3D 라벨 데이터가 부족("3D data barrier")**하다는 문제가 큼  

SAM 3D는 이걸 해결하기 위해 두 가지를 동시에 밀어붙임:

1. **모델**:  
   - 이미지 한 장에서 **geometry + texture + layout**을 같이 예측하는 두 단계 구조  
   - Geometry model + Texture & Refinement model (flow-matching 기반)  

2. **데이터 엔진**:  
   - 사람과 여러 3D 모델이 같이 도는 **human- & model-in-the-loop pipeline**  
   - 거의 **100만 장 가까운 real image + 3D annotation**을 만들어서 학습에 사용  

그 결과:

- **복잡한 자연 이미지**에서
  - occlusion·clutter가 있어도 꽤 그럴듯한 3D를 뽑아내고
- 사람 평가 기준으로도  
  - 기존 SOTA 대비 **최소 5:1 수준으로 선호**되는 결과를 보고함.  

참고로, 공식 그림은 이런 느낌 (다운받아서 써도 됨):

![SAM 3D overview](https://github.com/facebookresearch/sam-3d-objects/raw/main/doc/intro.png)

([GitHub][1])

---

## 2. Related Work (특히 SAM 1/2/3와의 관계)

### 2.1 SAM 1: Segment Anything (이미지용 promptable segmentation)

* **아이디어**: 어떤 이미지가 와도, point/box/text 같은 prompt를 주면 **해당 영역을 바로 segmentation**해 주는 foundation model.([arXiv][2])
* **데이터 엔진**:

  * SA-1B: 11M images / 1B masks 수준의 거대한 segmentation 데이터셋
  * 사람 + 모델이 같이 돌아가면서 점점 annotation을 키워가는 loop 구조.([arXiv][2])
* **포인트**:

  * vision 쪽에서 “promptable segmentation”이라는 foundation task를 제안했고,
  * 이후 segmentation 관련 거의 모든 작업의 출발점이 됨.

### 2.2 SAM 2: 이미지 + 비디오 통합 segmentation

* **확장 방향**:

  * SAM 1은 “이미지 한 장”만 보던 모델
  * SAM 2는 “이미지 = 1프레임짜리 비디오”로 보고, **동영상까지 포함한 promptable visual segmentation (PVS)**로 확장.([arXiv][3])
* **특징**:

  * streaming memory가 있는 transformer 구조
  * 실시간 비디오 segmentation 가능
  * 비디오용 데이터 엔진까지 포함해서, “segment anything in videos”를 노림.([arXiv][3])

### 2.3 SAM 3: Open-vocabulary concept segmentation

* **타겟**:

  * SAM 1/2는 주로 “어디를 잘라줄지”에 집중,
  * SAM 3는 “**무엇을** 자를지”까지 확장된 **open-vocabulary concept segmentation** 모델.([arXiv][4])
* **데이터 엔진**:

  * 4M+ unique concept, 270K concept가 들어간 SA-CO 벤치마크 등
  * 자동 annotation + human filtering으로 만들어진 거대한 개념 데이터셋.([arXiv][4])
* **기능**:

  * “vehicle” 같은 text prompt 하나로 이미지/비디오에서 해당 개념의 모든 인스턴스 segmentation & tracking.([Ultralytics Docs][5])

### 2.4 SAM 3D가 이 흐름에서 어디 서 있는가?

* SAM 3D는 **SAM 3를 앞단 segmentation 모듈로 쓰는 “3D 버전”**이라고 보면 편함:

  * SAM 3: “이미지/비디오 안에서 ‘무엇’을 segmentation 할지”
  * SAM 3D: “segmentation 된 그 객체를 **3D asset**으로 뽑아서 geometry/texture/layout까지 만드는 역할”([sam3d][6])
* SAM 패밀리 전체가:

  * **2D segmentation** → **video + concept** → **3D reconstruction**
    이렇게 점점 범위를 넓혀가는 그림 안에 SAM 3D가 위치.

---

## 3. Method (파이프라인 & 모델 구조)

여기서부터는 “이미지 한 장 → 3D 오브젝트”가 실제로 어떻게 되는지를 단계별로 정리.

### 3.1 Input / Output 정리

* **Input**

  * RGB image 1장
  * SAM 3로 얻은 object mask (text prompt나 클릭으로 선택)([sam3d][6])
  * (옵션) depth / point map (센서 또는 depth estimator로 추정한 2.5D 정보)

* **Output**

  * **Geometry**: coarse voxel shape → refined mesh / gaussian splat
  * **Texture**: 표면 텍스처
  * **Layout**: object의 회전(R), 평행이동(t), scale(s) — 월드 좌표에서 어디에 있는지

### 3.2 전체 파이프라인 한 번에 보기

공식 아키텍처 그림 (다운로드 가능):

![SAM 3D architecture](https://github.com/facebookresearch/sam-3d-objects/raw/main/doc/arch.png)

([GitHub][1])

흐름은 크게 네 단계:

1. **Segmentation**

   * SAM 3로 이미지에서 관심 객체 마스크를 얻음.

2. **Encoding**

   * 마스크된 객체 crop + 전체 이미지 context를

     * DINOv2 기반 이미지 인코더로 feature 추출 (image tokens).([alphaxiv.org][7])
   * depth / point map이 있으면 point encoder로 point tokens 생성.

3. **Geometry Model**

   * image tokens + (optional) point tokens + 랜덤 초기 shape/layout tokens를 받아
   * **coarse voxel shape + pose + scale**을 예측.

4. **Texture & Refinement Model**

   * Geometry output (shape latent) + image tokens + mask를 입력으로
   * high-res mesh / gaussian splat 형태의 geometry + texture를 생성.([alphaxiv.org][7])

### 3.3 Geometry Model (coarse shape & layout)

**입력 토큰**

* image tokens: DINOv2 encoder 출력
* point tokens (옵션): point map / depth encoder 출력
* shape tokens: “이 object의 3D 모양”을 담는 latent
* layout tokens: “이 object가 3D 공간 어디에 있는지”를 담는 latent([alphaxiv.org][7])

**코어 구조 – Mixture of Transformers**

* **Shape stream**

  * shape tokens 중심으로 돌아가는 transformer
* **Layout stream**

  * layout tokens 중심으로 돌아가는 transformer
* 중간마다 image/point tokens과 cross-attention 하면서

  * “이미지에서 얻은 단서”를 가져오고
  * shape ↔ layout 사이 정보도 교환

직관적으로는:

* shape stream: 이 물체의 전체적인 3D 형태를 상상하는 역할
* layout stream: 카메라 기준으로 어디에 있고, 어떤 방향을 보고 있는지 정리하는 역할

**출력**

* Shape decoder → 3D voxel grid (coarse)
* Layout decoder → rotation, translation, scale 파라미터([alphaxiv.org][7])

여기까지가 “대충 덩어리를 만들어서 어디에 둘지 결정하는 단계”라고 보면 됨.

### 3.4 Texture & Refinement Model (고해상도 geometry + texture)

**입력**

* image tokens (이미지 디테일)
* object mask
* Geometry Model에서 나온 shape latent / coarse voxel

**코어 구조 – Latent Flow Transformer**

* **rectified flow matching** 기반의 generative transformer:

  * diffusion처럼 noise → data로 가는 경로를 학습하는데,
  * 여기서는 **flow** 형태로 latent를 점점 refinement 하는 방식.([alphaxiv.org][7])

**출력**

* Mesh decoder:

  * 삼각형 mesh + texture UV 등 high-res geometry 복원
* Gaussian splat decoder:

  * 최근 유행하는 3D Gaussian splatting 표현
  * 각 point에 위치·scale·opacity·color가 붙어 있어, 빠르게 렌더링 가능.([GitHub][1])

즉,

> Geometry Model이 “깎기 전 덩어리”를 만들면,
> Texture & Refinement Model이 “세밀하게 조각하고 칠하는” 단계라고 보면 됨.

### 3.5 Data Engine & Training (Synthetic → Real + Preference)

SAM 3D가 꽤 공격적으로 주장하는 부분이 바로 이 **학습 레시피**.

#### (1) Synthetic pretraining

* 대규모 synthetic / CAD 3D object들로 pretrain.
* 깨끗한 환경에서 shape, pose 분포를 넓게 학습.([alphaxiv.org][7])

#### (2) Semi-synthetic / Render-paste mid-training

* 준비된 3D 객체를 **자연 이미지 배경에 렌더해서 붙이는(render-paste)** 방식.
* 다양한 augmentation:

  * flying occluders: 다른 object로 일부 가리는 연출
  * object swap: 같은 자리에서 다른 object로 교체
* 목표:

  * occlusion, clutter, 다양한 배경에 robust해지도록 학습.([alphaxiv.org][7])

#### (3) Real-world SFT + Human Preference (DPO)

* human- & model-in-the-loop 데이터 엔진으로 **real 사진 + 3D mesh + pose** pair를 만듦:

  * 여러 3D 모델이 동일 object에 대해 candidate mesh들을 생성
  * 사람 annotator가 **best-of-N** 방식으로 “가장 좋은 recon”을 선택
  * 어려운 케이스는 3D 아티스트가 직접 mesh 제작([alphaxiv.org][7])
* 이걸 이용해:

  * Supervised Fine-tuning(SFT)
  * 사람 선택 결과를 pair로 만들고 **Direct Preference Optimization(DPO)**로

    * 사람이 더 좋아한 recon의 확률 ↑
    * 덜 좋은 recon의 확률 ↓

요약하면:

> **Synthetic → Semi-synthetic → Real + Human preference**
> 3단계로 점점 “현실 세계에 맞게 정렬”시키는 training recipe.

---

## 4. Results & Evaluation

### 4.1 새 벤치마크: Artist Objects / SA-3DAO

* 논문과 프로젝트에서 **Artist Objects** / **SAM 3D Artist Objects (SA-3DAO)** 계열의 벤치마크를 소개:

  * 자연 이미지 + 전문가(3D 아티스트)가 만든 high-quality 3D mesh로 구성
  * occlusion·clutter·복잡한 카테고리를 적극적으로 포함([alphaxiv.org][7])
* 목적:

  * 기존 synthetic/간단 object 중심 dataset보다,
    **실제 사용 상황에 가까운 3D reconstruction 난이도**를 제공.

### 4.2 정량적 성능

알파Xiv·HF 요약 기준 핵심만 뽑으면:([alphaxiv.org][7])

* SA-3DAO 등 in-the-wild 벤치마크에서,

  * 기존 image-to-3D / single-view 3D 방법들보다
    Chamfer distance, IoU, pose 관련 지표 모두 의미 있게 개선.
* 특히,

  * occlusion이 심한 경우
  * 배경이 복잡한 경우
    에서 gap이 더 크게 난다고 보고.

수치 하나하나보다는, 전반적으로 **모든 주요 지표에서 SOTA**를 찍었다는 게 포인트.

### 4.3 Human Preference 실험

논문/요약에서 push하는 “킬링 포인트”는 이거:([alphaxiv.org][7])

* 사람에게 **기존 방법 vs SAM 3D** 결과를 보여주고, 어느 쪽이 더 좋은지 투표.
* 결과:

  * **object-level**: 최소 **5:1** 비율로 SAM 3D 쪽이 선호됨.
  * **scene-level**: 약 **6:1** 비율로 SAM 3D 선호.

즉, 정량 지표뿐만 아니라 **실제 사람이 봐도 훨씬 낫다**는 걸 꽤 공격적으로 보여줌.

---

## 5. Conclusion (내 생각 + 다음에 볼 논문)

### 5.1 내 생각 (정리)

1. **문제 설정이 진짜 실용적**

   * “단일 이미지 → 원하는 object만 3D asset으로 뽑기”
   * 이건 그냥 바로 **콘텐츠 제작, e-commerce, AR/VR**에 꽂을 수 있는 플로우라, 연구 + 프로덕트 둘 다 잡는 느낌.

2. **모델 + 데이터 엔진을 같이 보는 게 핵심**

   * 그냥 “새로운 아키텍처”만 내는 논문이 아니라,

     * synthetic → semi-synthetic → real + preference
       까지 포함한 전체 파이프라인을 끝까지 밀어붙임.
   * 이런 식의 **data-centric + model-centric** 조합은
     나중에 UCDIR / domain gap 줄이는 쪽에도 거의 그대로 아이디어를 가져올 수 있을 듯.

3. **토큰 분리 아이디어가 인상적**

   * shape token vs layout token을 분리해서,

     * “geometry”와 “scene position”을 따로 관리하는 구조.
   * 나중에 내 쪽에서는

     * style token vs content token
     * domain token vs class token
       이런 식으로 나눠서 domain-invariant representation 만드는 데 써먹을 수 있을 것 같음.

4. **엄청난 리소스 의존이라는 현실적인 한계**

   * 이 스케일의 data engine + human preference loop는
     솔직히 말하면 big tech 스타일.
   * 하지만 “레시피” 자체는 좋은 참고가 되니까,
     **더 가벼운 버전**을 만들 수 있을지 고민해볼 가치가 있음.

### 5.2 나중에 구현/연구에 써먹을 포인트

* **UCDIR / domain gap reduction 쪽으로 옮기기**

  * render-paste 스타일 augmentation으로 background/domain 바꾸기
  * 여러 representation/head ensemble → best-of-N retrieval을 사람이 고르게 하고,
    그걸 preference pair로 만들어 DPO-style로 alignment 하는 구조.
* **2D feature에서도 token 분리**

  * style vs content token 분리 + cross-attention 구조
  * domain-adversarial 대신 이런 구조적 분리로 domain invariance를 노릴 수 있음.

### 5.3 다음에 같이 리뷰하면 좋은 논문 후보

SAM 3D 바로 위에 있는 애가 **SAM 3: Segment Anything with Concepts (2511.16719)**라서, 다음 논문으로 이걸 보는 게 자연스러워 보임:([arXiv][4])

* 이유:

  * SAM 3는 **4M+ 개념, 270K concepts**가 들어간 open-vocabulary segmentation foundation model.
  * SAM 3D는 이 segmentation 능력을 그대로 받아서 “3Dfy”하는 버전이라,

    * SAM 3의 data engine / architecture를 먼저 정리해 두면
      SAM 3D 전체 흐름이 더 깔끔하게 연결됨.
* 다음 리뷰에서 포인트:

  * SAM 1 → SAM 2 → SAM 3로 가면서
    *task가 어떻게 커지는지* (image → video → open-vocab concept)
  * SAM 3의 data engine (4M concept)과
    SAM 3D의 3D data engine을 비교해서 보는 것도 재밌을 듯.

원하면 다음에는 **“SAM 3: Segment Anything with Concepts”**를
이번이랑 비슷한 느낌으로 intro / related / method / result / conclusion 구조로 정리해볼게.


[1]: https://github.com/facebookresearch/sam-3d-objects?utm_source=chatgpt.com "GitHub - facebookresearch/sam-3d-objects: SAM 3D Objects"
[2]: https://arxiv.org/abs/2304.02643?utm_source=chatgpt.com "[2304.02643] Segment Anything - arXiv.org"
[3]: https://arxiv.org/abs/2408.00714?utm_source=chatgpt.com "[2408.00714] SAM 2: Segment Anything in Images and Videos"
[4]: https://arxiv.org/abs/2511.16719?utm_source=chatgpt.com "[2511.16719] SAM 3: Segment Anything with Concepts - arXiv.org"
[5]: https://docs.ultralytics.com/models/sam-3/?utm_source=chatgpt.com "SAM 3: Segment Anything with Concepts - Ultralytics YOLO Docs"
[6]: https://sam3d.world/?utm_source=chatgpt.com "sam3d | 3D World Image"
[7]: https://www.alphaxiv.org/overview/2511.16624v1?utm_source=chatgpt.com "SAM 3D: 3Dfy Anything in Images - alphaXiv"
